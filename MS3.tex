% Created 2024-12-12 Thu 23:31
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{braket}
\usepackage{tcolorbox}
\usepackage{physics}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage[margin=0.5in]{geometry}
\usepackage[normalem]{ulem}
\setcounter{secnumdepth}{0}
\author{Haadi Khan, Domenic Fioveranti, Jiming Chen, Will Bradley}
\date{\today}
\title{3110 MS3 Report}
\hypersetup{
 pdfauthor={Haadi Khan, Domenic Fioveranti, Jiming Chen, Will Bradley},
 pdftitle={3110 MS3 Report},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 30.0.92 (Org mode 9.8)}, 
 pdflang={English}}
\begin{document}

\maketitle
\section{Vision}
\label{sec:org45aef29}
Our vision is to implement a transformer architecture-based neural network trained on high-performing (i.e. highly-upvoted) posts on the popular social media app “Sidechat.” The posts will be gathered specifically from Sidechat’s Cornell community, which will encourage our model to output Cornell-centric posts. As a test of the model’s abilities, our group will make a series of posts to the app using text solely generated from our model, in the hope that the model can mimic an entertaining, human Cornell student and thus garner a positive following. We ran into a minor technical difficulty since MS2, namely our compute power. So, our hope of a high-performing sidechat account has since changed, but the rest of our vision is the same.
\section{Summary}
\label{sec:org94c1f6a}
For MS2, Domenic created the initial tokenizer (modeled after Google's wordpiece) for the system. However, we ran into some technical issues with its training stage and decided to abandon it after demoing it in MS2. JJ created the initial transformer stack, including the tokenizer, transformer, and pretraining. Haadi refined the transformer, made it more modular, and created a moderation system for the model. Will optimized the transformer code for pretraining to utilize our limited resources more effectively and wrote tests for matrix. Domenic finished the test suite and built the fully-featured TUI.

A notable change from MS2 is that we decided to abandon the OCaml Torch library. We encountered considerable difficulty with installation for our group. Despite our eventual successful installation, we decided to avoid the burden for our graders, and the required installation of libtorch.
\section{Activity Breakdown}
\label{sec:orgcfd1bb0}
\subsection{Domenic Fioravanti (dmf252)}
\label{sec:orgf8d260c}
\begin{itemize}
\item Researched transformer architecture and planned model implementation
\item Developed extensive test suite for the project
\item Built our refined TUI
\item 24 hours
\end{itemize}
\subsection{Haadi Khan (hmk68)}
\label{sec:org5f91cba}
\begin{itemize}
\item Built the initial driver for the end-user to interact with
\item Group planning and role assignment
\item Refined transformer model and created a content moderation system
\item Aided with test suite + final TUI
\item 25 hours
\end{itemize}
\subsection{Jiming Chen (jc3579)}
\label{sec:orgec1db26}
\begin{itemize}
\item Did data wrangling by finding the Sidechat API and made 5,000 API calls to retrieve 86,980 text posts along with their upvote count, comment count, and timestamps
\item Built initial transformer architecture
\item Worked on pretraining
\item 26 hours
\end{itemize}
\subsection{Will Bradley (wjb247)}
\label{sec:org63815ce}
\begin{itemize}
\item Optimized the training process
\item Worked with data sets to optimize our model performance
\item Made minor contributions to driver
\item 25 hours
\end{itemize}
\section{Productivity analysis}
\label{sec:orgea04c50}
We were efficient during MS3. In our sprints we developed the transformer relatively fast and did a good job overall. The main issue we ran into was compute. Unfortunately, we did not have adequate compute resources to utilize GPUs to speedup our training process which resulted in sub-par performance in the comprehensibility of our eventual model. If we constructed our transformer earlier in the semester we might have been able to do a better job with our project.

That being said, every person in the group pulled their weight and did a good job with their respective responsibilities. We did everything we sought out to do at the beginning of the semester, and our only issue was a lack of compute power for our model's training phase.
\end{document}
